{
  "llmModels": [
    {
      "id": "openai-gpt4o",
      "provider": "OpenAI",
      "model": "gpt-4o",
      "name": "GPT-4o",
      "enabled": true,
      "parameters": {
        "temperature": 0.7,
        "maxTokens": 500,
        "systemPrompt": "You are a helpful assistant providing clear, factual information."
      }
    },
    {
      "id": "anthropic-claude3.7sonnet",
      "provider": "Anthropic",
      "model": "claude-3-7-sonnet-20250219",
      "name": "Claude 3.7 Sonnet",
      "enabled": true,
      "parameters": {
        "temperature": 0.7,
        "maxTokens": 500,
        "systemPrompt": "You are Claude, a helpful AI assistant created by Anthropic."
      }
    },
    {
      "id": "google-gemini2.0",
      "provider": "Google",
      "model": "gemini-2.0-flash",
      "name": "Gemini 2.0 Flash",
      "enabled": true,
      "parameters": {
        "temperature": 0.7,
        "maxTokens": 500,
        "systemPrompt": "You are Gemini, a helpful AI assistant created by Google."
      }
    },
    {
      "id": "perplexity-sonar-pro",
      "provider": "Perplexity",
      "model": "sonar-pro",
      "name": "Perplexity Sonar Pro",
      "enabled": true,
      "parameters": {
        "temperature": 0.7,
        "maxTokens": 500,
        "systemPrompt": "You are a helpful assistant."
      }
    }
  ],
  "analyzerConfig": {
    "spontaneous": {
      "runsPerModel": 1,
      "fallback": {
        "provider": "Anthropic",
        "model": "claude-3-7-sonnet-20250219"
      },
      "primary": {
        "provider": "OpenAI",
        "model": "gpt-4o"
      }
    },
    "sentiment": {
      "fallback": {
        "provider": "Anthropic",
        "model": "claude-3-7-sonnet-20250219"
      },
      "primary": {
        "provider": "OpenAI",
        "model": "gpt-4o"
      }
    },
    "comparison": {
      "fallback": {
        "provider": "Anthropic",
        "model": "claude-3-7-sonnet-20250219"
      },
      "primary": {
        "provider": "OpenAI",
        "model": "gpt-4o"
      }
    }
  },
  "concurrencyLimit": 100,
  "pipelineLimits": {
    "spontaneous": 150,
    "sentiment": 100,
    "comparison": 80
  },
  "retryConfig": {
    "maxRetries": 3,
    "baseDelayMs": 1000,
    "maxDelayMs": 30000,
    "backoffFactor": 2.0,
    "description": "Retry configuration for LLM API calls when rate limited"
  }
}
